{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611e9489-6fef-4be5-93e2-d9a5a1a7d3ce",
   "metadata": {},
   "source": [
    "#### How to work with this script - \n",
    "I assume that the new dataset is in the same structure as the embeddings supplied thus far. <br>\n",
    "To run the script you should change the path from which you read the embeddings files, and the path of the trained model. <br>\n",
    "You can get 2 outputs  - <br>\n",
    "A. The predicted face embedding derived from the audio inputs by the trained model(predicted_emb below) <br>\n",
    "B. An estimation of the identification accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17e8e1-5dd1-4e4e-a4cb-c79d7a192ca5",
   "metadata": {},
   "source": [
    "#### 1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59173387-2f20-4620-aaf4-db53eb1ff6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle, random\n",
    "from pandasql import sqldf\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c8517-be8c-4386-ac10-3e8d9e7de1c1",
   "metadata": {},
   "source": [
    "#### 2. Load data, and transfer to pands dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa3d78-f76d-427a-9521-1bedfe4cb7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_emb_file = '/home/drorco/DC_corsound_submission/audio_embeddings.pickle'\n",
    "face_emb_file = '/home/drorco/DC_corsound_submission/image_embeddings.pickle'\n",
    "\n",
    "audio_emb = pickle.load(open(audio_emb_file, \"rb\"))\n",
    "face_emb = pickle.load(open(face_emb_file, \"rb\"))\n",
    "\n",
    "audio_df =  pd.DataFrame.from_dict(audio_emb).T.reset_index()\n",
    "face_df =  pd.DataFrame.from_dict(face_emb).T.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2ad2e-f276-4340-867a-4df3befcdd84",
   "metadata": {},
   "source": [
    "#### 3. Get the identity (Name) in each modality and generate the mean face embedding for each name\n",
    "It appears that the Name (ID) appears in the string inside the 'index', just before the first slash. <br> After the slash comes the identifier of the file. <br> Hence I split the index in both modalities before the first slash, and give a regular numbering for the files of each ID.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc7636-e522-4ead-aad8-7a48977ab62f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_df[['s_Name','s_file']] = audio_df['index'].str.split('/', n=1, expand=True)\n",
    "audio_df = audio_df.sort_values(by=['s_Name','index'])\n",
    "audio_df['s_audio_fNum'] = audio_df.groupby('s_Name')['index'].rank(method='first').astype('int')\n",
    "audio_df = audio_df.drop(columns=['index','s_file'])\n",
    "audio_df= audio_df.set_index(['s_Name','s_audio_fNum']).add_prefix('audio_col_').reset_index()\n",
    "\n",
    "audio_cols = [col for col in audio_df.columns if 'col_' in col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe5c25-e35c-4102-a0b9-2a2c1a55b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_df[['s_Name','s_file']] = face_df['index'].str.split('/', n=1, expand=True)\n",
    "face_df = face_df.sort_values(by=['s_Name','index'])\n",
    "face_df['s_face_fNum'] = face_df.groupby('s_Name')['index'].rank(method='first').astype('int')\n",
    "face_df = face_df.drop(columns=['index','s_file'])\n",
    "face_df= face_df.set_index(['s_Name','s_face_fNum']).add_prefix('face_col_').reset_index()\n",
    "\n",
    "face_cols = [col for col in face_df.columns if 'col_' in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddfd36a-0612-4f65-b160-a9b5dbfaa98a",
   "metadata": {},
   "source": [
    "#### 4. load the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0cb5e-3bda-4426-b1cb-424188d3a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/drorco/DC_corsound_submission/checkpnt1.pkl'\n",
    "loaded_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a16a3-6cf0-44c3-99c2-9df5fc5dd0fd",
   "metadata": {},
   "source": [
    "#### 5. Create a predicted face embedding for each of the audio embeddings in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062360ab-afdf-4be4-a6b6-1ca762707b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_emb = audio_df[['s_Name','s_audio_fNum']]\n",
    "\n",
    "face_cols_preds = [col+'_pred' for col in face_cols]\n",
    "\n",
    "predicted_emb[face_cols_preds] = loaded_model.predict(audio_df[audio_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974328e-e3f1-4c86-a1cf-11582742d2a2",
   "metadata": {},
   "source": [
    "#### 6. Evaluate the identification accuracy\n",
    "Here for each of the PREDICTED face embeddings (Anchor) is tested against a random positive and negative Actual face embedding samples.\n",
    "A positive sample - one of the face embeddings of the same ID as the original audio embedding upon which the prediction was made.\n",
    "A negative sample - one of the face embeddings of a different ID.\n",
    "If it is closer to the positive sample it is considered as Correct identification .\n",
    "If it is closer to the negative sample it is considered as False identification.\n",
    "The accuracy is the percentage of correct identifications out of the entire test dataset.\n",
    "For the sake of time, I choose not to generate all the possible combinations of audio-inputs, positive-face-embeddings, and negative-face embeddings as in the original identification accuracy metric. Instead, I generated predicted face embeddings from all audio-inputs, and compared them with randomly selected samples of positive and negative face embeddings as approximation of the identification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3972710-1b85-4d3b-88ae-1db6d00a7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = predicted_emb['s_Name'].unique().tolist()\n",
    "l=[]\n",
    "\n",
    "for name in name_list:\n",
    "    if name in (face_df['s_Name'].unique().tolist()):\n",
    "        \n",
    "        rndom_l = [rnd_name for rnd_name in name_list if rnd_name!=name]\n",
    "        rndom = random.choice(rndom_l)\n",
    "        \n",
    "        positive = face_df[(face_df['s_Name']==name)]\n",
    "        negative = face_df[(face_df['s_Name']==rndom)]\n",
    "\n",
    "        anchor = predicted_emb[predicted_emb['s_Name']==name].sort_values(by=['s_Name','s_audio_fNum'])\n",
    "        \n",
    "        if ((positive.shape[0]>0) and (anchor.shape[0]>0)):\n",
    "\n",
    "            for audio_fNum in anchor['s_audio_fNum'].unique().tolist():\n",
    "                \n",
    "                curr_pair = positive.sample().append(negative.sample())\n",
    "                            \n",
    "                knn = KNeighborsClassifier(n_neighbors=1)\n",
    "                knn.fit(curr_pair[face_cols], curr_pair['s_Name'])               \n",
    "                \n",
    "                pred_name = knn.predict(anchor[anchor['s_audio_fNum']==audio_fNum][face_cols_preds])\n",
    "                \n",
    "                l.append(int(pred_name==anchor[anchor['s_audio_fNum']==audio_fNum]['s_Name']))\n",
    "                \n",
    "print('The Accuracy is: ', round(sum(l)/len(l),4)*100, '%')                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca713c9-2f25-482b-b287-a0c9fedfdbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9b26f-d38d-480f-bc38-b29088207559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25865627-25cf-4e12-997d-488a42aa33ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558f727-d7d2-42c1-87a1-3e1fd458c4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56183df-6a07-4229-a5aa-9d0035d5f7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fbe200-6688-44ba-a45a-30fe9ce82e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adcd8f-c1a3-4971-b9ce-287472e94847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493aac03-0e35-46bb-a870-3bd1092da296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddd5fd-4b04-46f6-9dd6-fc11a2e69ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
